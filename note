1.
	If make the condition = (#click>10) , then
		(#session) / (#buy) ~= 4
	but overall is
		(#session) / (#buy) ~= 50
	SO it's a strong condition.

2. 
	Using feature "month/hour bit" with liblinear on {#click>10}
	use 300000 to train, 50000 to valid:
		correctness: 58%
		           predict 0    predict 1
		answer 0      24607       13716
		answer 1       7031        4646
	Which is doing nothingQQ 

3. 
	Using feature "month/cate bit" with liblinear on {#click>10}
	use 300000 to train, 50000 to valid:
		correctness: 62%
		             predict 0   predict 1
	         answer 0      26666      11657
	         answer 1      7376        4301
	Why??? cate should no effect to result?

4.
    Weighting in liblinear is useless orz.
    Weighting only scale #(predict 1), but while scaling,
    .#(predict 1 but is zero) increase with same ratio.

5.
    Using feature "cate bit" with liblinear on {#click>10}:
        Accuracy = 67.234% (33617/50000)
                  predict 0   predict 1
        answer 0      30308        8015
        answer 1       8368        3309
    Best result till now.
    Still get negetive score through.

6. 
	Using feature "is special offer or not" with liblinear on {#click>10}
        Accuracy = 69.076% (34538/50000)
              predict 0   predict 1
        answer 0      31836        6487
        answer 1       8975        2702
        (a1 p1) / (a0 p1) = 0.416525   (>1 then get positive score)
    Best result ever.
    So features are too much? In 3. 5. 6., we can see this tendency.

7.
	Using feature "is special offer or not" with liblinear on {#click>5}
        Accuracy = 74.1615% (148323/200000)
                  predict 0   predict 1
        answer 0     141602       27385
        answer 1      24292        6721
        (a1 p1) / (a0 p1) = 0.245426   (>1 then get positive score)
    Result turned bad if we extend the train/test data.
    Because (buy/all) become smaller (1/4->1/6).
    The real accuracy is stay unchanged.
    (That is, the ratio of (a1 p1)/(a0 p1) * (buy/all) is unchanged)
    So, NOT features too much, but other reason.
    
    

