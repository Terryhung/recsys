1.
	If make the condition = (#click>10) , then
		(#session) / (#buy) ~= 4
	but overall is
		(#session) / (#buy) ~= 50
	SO it's a strong condition.

2. 
	Using feature "month/hour bit" with liblinear on {#click>10}
	use 300000 to train, 50000 to valid:
		correctness: 58%
		           predict 0    predict 1
		answer 0      24607       13716
		answer 1       7031        4646
	Which is doing nothingQQ 

3. 
	Using feature "month/cate bit" with liblinear on {#click>10}
	use 300000 to train, 50000 to valid:
		correctness: 62%
		             predict 0   predict 1
	         answer 0      26666      11657
	         answer 1      7376        4301
	Why??? cate should no effect to result?

4.
    Weighting in liblinear is useless orz.
    Weighting only scale #(predict 1), but while scaling,
    .#(predict 1 but is zero) increase with same ratio.

5.
    Using feature "cate bit" with liblinear on {#click>10}:
        Accuracy = 67.234% (33617/50000)
                  predict 0   predict 1
        answer 0      30308        8015
        answer 1       8368        3309
    Best result till now.
    Still get negetive score through.

6. 
	Using feature "is special offer or not" with liblinear on {#click>10}
        Accuracy = 69.076% (34538/50000)
              predict 0   predict 1
        answer 0      31836        6487
        answer 1       8975        2702
        (a1 p1) / (a0 p1) = 0.416525   (>1 then get positive score)
    Best result ever.
    So features are too much? In 3. 5. 6., we can see this tendency.

7.
	Using feature "is special offer or not" with liblinear on {#click>5}
        Accuracy = 74.1615% (148323/200000)
                  predict 0   predict 1
        answer 0     141602       27385
        answer 1      24292        6721
        (a1 p1) / (a0 p1) = 0.245426   (>1 then get positive score)
    Result turned bad if we extend the train/test data.
    Because (buy/all) become smaller (1/4->1/6).
    The real accuracy is stay unchanged.
    (That is, the ratio of (a1 p1)/(a0 p1) * (buy/all) is unchanged)
    So, NOT features too much, but other reason.
    
8. (important) - better refine?
    by data_analysis, know if we delete item with (#click<512),
    we only delete 1/10 data from the dataset.
    Which we can reduce #data 52739->7879.
    then purity(?) of the data is 9/10*6.6 ~= 6
    we will got 6 times compact data with enough features to train.

9.  a1/all = 0.226475 , (a1 p1) / (a0 p1) = 0.961202
    Using feature "cate bits" with liblinear on refine(8, 512)
        Accuracy = 77.2362% (61789/80000)    
                  predict 0   predict 1
        answer 0      59485        2397
        answer 1      15814        2304
        (a1 p1) / (a0 p1) = 0.961202   (>1 then get positive score)
        #answer 1 = 18118
        a1/all = 0.226475
    After deleting redundence items, we reached score 0! orz
    A big improve.
    * I can't achieve this twice. I picked wrong file to train/test?

10. a1/all = 0.222587 , (a1 p1) / (a0 p1) = 0.380659
    Using feature "spec offer" with liblinear on refine(8, 512)
        Accuracy = 72.4575% (57966/80000)
                  predict 0   predict 1
        answer 0      55368        6825
        answer 1      15209        2598
        (a1 p1) / (a0 p1) = 0.380659   (>1 then get positive score)
        #answer 1 = 17807
        a1/all = 0.222587
    only "spec offer" is bad this time.

11. Let's see influence of "better refine".
    All using feature "cate bits" with liblinear

    a1/all = 0.249695 , (a1 p1) / (a0 p1) = 0.760697 , refine(8, 4096)
        Accuracy = 73.8805% (40604/54959)
                  predict 0   predict 1
        answer 0      38595        2641
        answer 1      11714        2009
        (a1 p1) / (a0 p1) = 0.760697   (>1 then get positive score)
        #answer 1 = 13723
        a1/all = 0.249695

    a1/all = 0.307224 , (a1 p1) / (a0 p1) = 0.648188 , refine(32, 512)
        Accuracy = 62.5812% (1542/2464)
                  predict 0   predict 1
        answer 0       1238         469
        answer 1        453         304
        (a1 p1) / (a0 p1) = 0.648188   (>1 then get positive score)
        #answer 1 = 757
        a1/all = 0.307224

    a1/all = 0.215696 , (a1 p1) / (a0 p1) = 0.626311 , refine(8, 512)
        Accuracy = 76.3994% (73730/96506)
                  predict 0   predict 1
        answer 0      70445        5245
        answer 1      17531        3285
        (a1 p1) / (a0 p1) = 0.626311   (>1 then get positive score)
        #answer 1 = 20816
        a1/all = 0.215696

    a1/all = 0.136358 , (a1 p1) / (a0 p1) = 0.431380 , refine(4, 512)
        Accuracy = 85.1084% (286155/336224)
                  predict 0   predict 1
        answer 0     282952        7425
        answer 1      42644        3203
        (a1 p1) / (a0 p1) = 0.431380   (>1 then get positive score)
        #answer 1 = 45847
        a1/all = 0.136358

12. About refine and #buy
    no refine
        There are 1259710 sessions with #click<=2. Have 22363 session buys. (ratio:1.775%)
        There are 4817797 sessions with #click<=3. Have 132320 session buys. (ratio:2.746%)
        There are 7367044 sessions with #click<=4. Have 263666 session buys. (ratio:3.579%)
        There are 8272853 sessions with #click<=7. Have 343021 session buys. (ratio:4.146%)
        There are 8889767 sessions with #click<=10. Have 427024 session buys. (ratio:4.804%)
        There are 9107161 sessions with #click<=15. Have 471541 session buys. (ratio:5.178%)
        There are 9249727 sessions with #click<=259. Have 509696 session buys. (ratio:5.510%)
    refine(0, 512)
        There are 1461686 sessions with #click<=2. Have 30429 session buys. (ratio:2.082%)
        There are 4787874 sessions with #click<=3. Have 140656 session buys. (ratio:2.938%)
        There are 7169743 sessions with #click<=4. Have 270475 session buys. (ratio:3.772%)
        There are 8002937 sessions with #click<=7. Have 347533 session buys. (ratio:4.343%)
        There are 8556376 sessions with #click<=10. Have 427320 session buys. (ratio:4.994%)
        There are 8852719 sessions with #click<=259. Have 499628 session buys. (ratio:5.644%)
    refine(4, 512)
        There are 833193 sessions with #click<=7. Have 77058 session buys. (ratio:9.249%)
        There are 1386632 sessions with #click<=10. Have 156845 session buys. (ratio:11.311%)
        There are 1572588 sessions with #click<=15. Have 197503 session buys. (ratio:12.559%)
        There are 1644360 sessions with #click<=23. Have 217357 session buys. (ratio:13.218%)
        There are 1682975 sessions with #click<=259. Have 229153 session buys. (ratio:13.616%)
    So with no refine data, we can pick threshold = 2 => most buying are included.
    refine(0, 512) is good. Since it's merely deleted buying data with #feat/=4 .


